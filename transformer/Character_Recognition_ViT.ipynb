{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target is on all the root directory for all the _extracted folders\n",
    "dataset_dir = '../dataset' # Whatever folder the augmented dataset is in\n",
    "train_dir = '../processed_splits/train'  # Destination for training set\n",
    "test_dir = '../processed_splits/test'    # Destination for test set\n",
    "val_dir = '../processed_splits/val'    # Destination for validation set\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(dataset_dir, test_size, val_size):\n",
    "    for character_folder in os.listdir(dataset_dir):\n",
    "        character_path = os.path.join(dataset_dir, character_folder)\n",
    "\n",
    "        if os.path.isdir(character_path):\n",
    "            # Get all image filenames for the current character\n",
    "            images = os.listdir(character_path)\n",
    "\n",
    "            # Split into train+val and test sets\n",
    "            train_val_images, test_images = train_test_split(images, test_size=test_size, random_state=5242)\n",
    "\n",
    "            # Split train_val into train and val sets\n",
    "            train_images, val_images = train_test_split(train_val_images, test_size=val_size, random_state=5242)\n",
    "\n",
    "            # Create train, val, and test subfolders for the character\n",
    "            train_character_dir = os.path.join(train_dir, character_folder)\n",
    "            val_character_dir = os.path.join(val_dir, character_folder)\n",
    "            test_character_dir = os.path.join(test_dir, character_folder)\n",
    "            os.makedirs(train_character_dir, exist_ok=True)\n",
    "            os.makedirs(val_character_dir, exist_ok=True)\n",
    "            os.makedirs(test_character_dir, exist_ok=True)\n",
    "\n",
    "            # Move training images\n",
    "            for image in train_images:\n",
    "                src_image_path = os.path.join(character_path, image)\n",
    "                dst_image_path = os.path.join(train_character_dir, image)\n",
    "                shutil.copy(src_image_path, dst_image_path)\n",
    "\n",
    "            # Move validation images\n",
    "            for image in val_images:\n",
    "                src_image_path = os.path.join(character_path, image)\n",
    "                dst_image_path = os.path.join(val_character_dir, image)\n",
    "                shutil.copy(src_image_path, dst_image_path)\n",
    "\n",
    "            # Move testing images\n",
    "            for image in test_images:\n",
    "                src_image_path = os.path.join(character_path, image)\n",
    "                dst_image_path = os.path.join(test_character_dir, image)\n",
    "                shutil.copy(src_image_path, dst_image_path)\n",
    "\n",
    "            print(f'Successfully split {character_folder} into train/test/val sets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split ratios\n",
    "test_size = 0.15\n",
    "val_size = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_val_split(dataset_dir, test_size, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
    "val_dataset = torchvision.datasets.ImageFolder(val_dir, transform=transform)\n",
    "val_dataset = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "# Pass these datasets to DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False, maximize=False, foreach=None, capturable=False, differentiable=False, fused=None)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=30, patience=5):\n",
    "    best_val_accuracy = 0.0\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc='Training', leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs).logits\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_running_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc='Validation', leave=False):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs).logits\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                val_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_accuracy = val_running_corrects.double() / len(val_loader.dataset)\n",
    "        print(f'Validation Acc: {val_accuracy:.4f}')\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print('Best model saved.')\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            print(f'No improvement for {trigger_times} epoch(s).')\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f'Training complete. Best Validation Accuracy: {best_val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=30, patience=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
